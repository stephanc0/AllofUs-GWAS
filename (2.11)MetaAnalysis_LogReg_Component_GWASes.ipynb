{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Stephan Cordogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs a logistic regression across AllofUs genomic data for each of 5 ancestry sub-populations, using principal components calculated in notebooks 1.11 and 1.12.  The resulting summary statistics are saved to the workspace bucket.  For ease of computation (this can cost hundreds or thousands of dollars), the GWAS can be split up by chromosomes, run over each set of chromosomes, and recombined in the next notebook. This works better than splitting up only by ancestry, as memory requirements are drastically higher for the ancestries with higher populations.  Additionally, overall memory requirement is lower for the same volume of data when split by ancestry because each ancestry is iteratevely saved to a file.  **Simply specify the desired test_intervals** [(a)](#Split-up-by-chromosomes-if-desired) **and change the numerical suffix in the save path- you may also change the overall and ancestry-specific minor allele frequencies** [(b)](#Run-GWASes).  I recommend running chromosomes in these clusters [\"1\", \"2\", \"3\", \"45\", \"67\", \"89\", \"101112\", \"131415\", \"161718\", \"19202122\"].  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up GWASes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:41.481107Z",
     "iopub.status.busy": "2024-04-19T15:31:41.480225Z",
     "iopub.status.idle": "2024-04-19T15:31:41.486288Z",
     "shell.execute_reply": "2024-04-19T15:31:41.484516Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import hail as hl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:41.490973Z",
     "iopub.status.busy": "2024-04-19T15:31:41.490350Z",
     "iopub.status.idle": "2024-04-19T15:31:41.495824Z",
     "shell.execute_reply": "2024-04-19T15:31:41.494697Z"
    }
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "bucket\n",
    "hl.init\n",
    "hl.default_reference(\"GRCh38\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $WORKSPACE_BUCKET/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hail MatrixTable containing the variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:59.523403Z",
     "iopub.status.busy": "2024-04-19T15:31:59.523044Z",
     "iopub.status.idle": "2024-04-19T15:31:59.530240Z",
     "shell.execute_reply": "2024-04-19T15:31:59.528918Z"
    }
   },
   "outputs": [],
   "source": [
    "# mt_path = os.getenv(\"WGS_CLINVAR_SPLIT_HAIL_PATH\")\n",
    "mt_path = os.getenv(\"WGS_ACAF_THRESHOLD_SPLIT_HAIL_PATH\")\n",
    "# mt_path = os.getenv(\"WGS_ACAF_THRESHOLD_MULTI_HAIL_PATH\")\n",
    "mt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up by chromosomes if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:59.535853Z",
     "iopub.status.busy": "2024-04-19T15:31:59.534946Z",
     "iopub.status.idle": "2024-04-19T15:32:04.524427Z",
     "shell.execute_reply": "2024-04-19T15:32:04.522864Z"
    }
   },
   "outputs": [],
   "source": [
    "mt = hl.read_matrix_table(mt_path)\n",
    "# mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_intervals = ['chr6:54000000-57000000']\n",
    "# test_intervals = ['chr3','chr4','chr5','chr6', 'chr7', 'chr8']\n",
    "# test_intervals = ['chr6']\n",
    "test_intervals = ['chr19', 'chr20', 'chr21', 'chr22']\n",
    "# test_intervals = ['chr22']\n",
    "\n",
    "mt = hl.filter_intervals(\n",
    "    mt,\n",
    "    [hl.parse_locus_interval(x,)\n",
    "     for x in test_intervals])\n",
    "\n",
    "# mt_qual = hl.filter_intervals(\n",
    "#     mt_qual,\n",
    "#     [hl.parse_locus_interval(x,)\n",
    "#      for x in test_intervals])\n",
    "# mt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load phenotypic data and link with genomic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:32:49.983204Z",
     "iopub.status.busy": "2024-04-19T15:32:49.982572Z",
     "iopub.status.idle": "2024-04-19T15:32:49.988652Z",
     "shell.execute_reply": "2024-04-19T15:32:49.987668Z"
    }
   },
   "outputs": [],
   "source": [
    "phenotype_filename = f'{bucket}/data/genomics_phenotypes.tsv'\n",
    "phenotype_filename\n",
    "phenotypes = (hl.import_table(phenotype_filename,\n",
    "                              types={'person_id':hl.tstr},\n",
    "                              impute=True,\n",
    "                              key='person_id')\n",
    "             )\n",
    "mt = mt.semi_join_cols(phenotypes)\n",
    "mt = mt.annotate_cols(pheno = phenotypes[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the genomic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove related samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:33:35.690611Z",
     "iopub.status.busy": "2024-04-19T15:33:35.689900Z",
     "iopub.status.idle": "2024-04-19T15:33:35.694087Z",
     "shell.execute_reply": "2024-04-19T15:33:35.693235Z"
    }
   },
   "outputs": [],
   "source": [
    "related_samples_path = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv\"\n",
    "related_remove = hl.import_table(related_samples_path,\n",
    "                                 types={\"sample_id\":\"tstr\"},\n",
    "                                key=\"sample_id\")\n",
    "\n",
    "#related_remove.count()\n",
    "mt = mt.anti_join_cols(related_remove)\n",
    "#mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link predicted ancestry for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:34:17.745922Z",
     "iopub.status.busy": "2024-04-19T15:34:17.745281Z",
     "iopub.status.idle": "2024-04-19T15:34:17.749816Z",
     "shell.execute_reply": "2024-04-19T15:34:17.748869Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestry_pred_path = \"gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/ancestry/ancestry_preds.tsv\"\n",
    "ancestry_pred = hl.import_table(ancestry_pred_path,\n",
    "                               key=\"research_id\", \n",
    "                               impute=True, \n",
    "                               types={\"research_id\":\"tstr\",\"pca_features\":hl.tarray(hl.tfloat)})\n",
    "mt = mt.annotate_cols(ancestry_pred = ancestry_pred[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally generate summary statistics of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_table = mt.cols()\n",
    "# result_table = (\n",
    "#     col_table\n",
    "#     .group_by(col_table.ancestry_pred.ancestry_pred)  # Group by ancestry prediction\n",
    "#     .aggregate(\n",
    "#         cases=hl.agg.sum(col_table.pheno.has_pheno == 1),  # Count of cases\n",
    "#         controls=hl.agg.sum(col_table.pheno.has_pheno == 0),  # Count of controls\n",
    "#         num_males=hl.agg.sum(col_table.pheno.is_male == 1),  # Count of males\n",
    "#         num_females=hl.agg.sum(col_table.pheno.is_female),  # Count of females\n",
    "#         mean_age=hl.agg.mean(col_table.pheno.age_yrs)  # Mean age\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# result_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GWASes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_ancestries = hl.literal({\"eur\", \"afr\", \"amr\", \"eas\", \"sas\"})\n",
    "mt = mt.filter_cols(used_ancestries.contains(mt.ancestry_pred.ancestry_pred))\n",
    "\n",
    "# OVERALL MINOR ALLELE FREQUENCY THRESHOLD\n",
    "mt = mt.filter_rows(hl.min(mt.info.AF) > 0.001, keep=True)\n",
    "\n",
    "# Define PCA scores file paths and save paths for each ancestry\n",
    "pca_files = {\n",
    "    \"eur\": f'{bucket}/data/mt_eur_pcs.tsv.bgz',\n",
    "    \"afr\": f'{bucket}/data/mt_afr_pcs.tsv.bgz',\n",
    "    \"amr\": f'{bucket}/data/mt_amr_pcs.tsv.bgz',\n",
    "    \"eas\": f'{bucket}/data/mt_eas_pcs.tsv.bgz',\n",
    "    \"sas\": f'{bucket}/data/mt_sas_pcs.tsv.bgz'\n",
    "}\n",
    "\n",
    "# POPULATION SPECIFIC MINOR ALLELE FREQUENCY THRESHOLD\n",
    "allele_freq_thresholds = {\n",
    "    \"eur\": 0.001,\n",
    "    \"afr\": 0.005,\n",
    "    \"amr\": 0.005,\n",
    "    \"eas\": 0.01,\n",
    "    \"sas\": 0.01\n",
    "}\n",
    "\n",
    "# Run logistic regression for each ancestry\n",
    "for ancestry, pca_file in pca_files.items():\n",
    "    # Filter to specific ancestry and re-annotate rows\n",
    "    mt_ancestry = mt.filter_cols(mt.ancestry_pred.ancestry_pred == ancestry)\n",
    "    mt_ancestry = mt_ancestry.annotate_rows(info=hl.agg.call_stats(mt_ancestry.GT, mt_ancestry.alleles))\n",
    "    \n",
    "    min_af = allele_freq_thresholds[ancestry]\n",
    "    mt_ancestry = mt_ancestry.filter_rows(hl.min(mt_ancestry.info.AF) > min_af, keep=True)\n",
    "    \n",
    "    # Import and join PCA scores\n",
    "    mt_ancestry_pcs = hl.import_table(pca_file, impute=True)\n",
    "    mt_ancestry_pcs = mt_ancestry_pcs.annotate(s=hl.str(mt_ancestry_pcs.s))\n",
    "    mt_ancestry_pcs = mt_ancestry_pcs.key_by('s')\n",
    "    \n",
    "    # Annotate columns with PCA scores\n",
    "    mt_ancestry = mt_ancestry.annotate_cols(\n",
    "        pca_scores=hl.struct(\n",
    "            PC1=mt_ancestry_pcs[mt_ancestry.s].PC1,\n",
    "            PC2=mt_ancestry_pcs[mt_ancestry.s].PC2,\n",
    "            PC3=mt_ancestry_pcs[mt_ancestry.s].PC3,\n",
    "            PC4=mt_ancestry_pcs[mt_ancestry.s].PC4,\n",
    "            PC5=mt_ancestry_pcs[mt_ancestry.s].PC5,\n",
    "            PC6=mt_ancestry_pcs[mt_ancestry.s].PC6,\n",
    "            PC7=mt_ancestry_pcs[mt_ancestry.s].PC7,\n",
    "            PC8=mt_ancestry_pcs[mt_ancestry.s].PC8,\n",
    "            PC9=mt_ancestry_pcs[mt_ancestry.s].PC9,\n",
    "            PC10=mt_ancestry_pcs[mt_ancestry.s].PC10\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define covariates, including PCA scores\n",
    "    covariates = [1.0, mt_ancestry.pheno.is_male, mt_ancestry.pheno.age_yrs] + [\n",
    "        mt_ancestry.pca_scores[f'PC{i}'] for i in range(1, 11)\n",
    "    ]\n",
    "\n",
    "    # Perform logistic regression\n",
    "    log_reg = hl.logistic_regression_rows(\n",
    "        test='wald',\n",
    "        y=mt_ancestry.pheno.has_pheno,\n",
    "        x=mt_ancestry.GT.n_alt_alleles(),\n",
    "        covariates=covariates\n",
    "    )\n",
    "\n",
    "    # Flatten and export results\n",
    "    log_reg_flat = log_reg.flatten()\n",
    "    log_reg_save_path = f'{bucket}/data/log_reg_{ancestry}_19202122.tsv.bgz'\n",
    "    log_reg_flat.export(log_reg_save_path)\n",
    "\n",
    "    print(f\"Logistic regression for {ancestry} completed and saved to {log_reg_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
