{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Stephan Cordogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document stitches together the GWAS summary statistics generated in the previous notebook [(a)](#Merge-component-files-if-necessary) (if necessary), and meta- analyzes them together, as well as the processed finngen file, although this can be skipped if not applicable [(b)](#Load-in-Finngen-GWAS) [(c)](#Process-Finngen-if-desired), **or hashtagged out** [(d)](#Perform-meta-analysis-using-METAL).  You could also load in another summary statistics file if desired.  The resultant summary statistics are saved to your workspace bucket as meta_all1_GC.tsv.bgz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:41.481107Z",
     "iopub.status.busy": "2024-04-19T15:31:41.480225Z",
     "iopub.status.idle": "2024-04-19T15:31:41.486288Z",
     "shell.execute_reply": "2024-04-19T15:31:41.484516Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-19T15:31:41.490973Z",
     "iopub.status.busy": "2024-04-19T15:31:41.490350Z",
     "iopub.status.idle": "2024-04-19T15:31:41.495824Z",
     "shell.execute_reply": "2024-04-19T15:31:41.494697Z"
    }
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $WORKSPACE_BUCKET/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download METAL using wget in Jupyter\n",
    "!wget https://csg.sph.umich.edu/abecasis/Metal/download/Linux-metal.tar.gz\n",
    "    \n",
    "!tar -xvzf Linux-metal.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in GWAS Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge component files if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define  parameters\n",
    "ethnicities = [\"eur\", \"afr\", \"amr\", \"eas\", \"sas\"]\n",
    "file_ids = [\"1\", \"2\", \"3\", \"45\", \"67\", \"89\", \"101112\", \"131415\", \"161718\", \"19202122\"]\n",
    "file_extension = \".tsv\"\n",
    "\n",
    "def load_files(file_path, local_file_name):\n",
    "    subprocess.run([\"gsutil\", \"cp\", file_path, \".\"], check=True)\n",
    "    decompressed_file = local_file_name.rstrip(\".bgz\")\n",
    "    if os.path.exists(decompressed_file):\n",
    "        os.remove(decompressed_file)\n",
    "    subprocess.run([\"bgzip\", \"-d\", local_file_name], check=True)\n",
    "    return decompressed_file\n",
    "\n",
    "def clean_df(df):\n",
    "    # Drop unwanted columns and rows with NaN\n",
    "    return df.drop(columns=[\"fit.n_iterations\", \"fit.converged\", \"fit.exploded\"], errors=\"ignore\").dropna()\n",
    "\n",
    "merged_dataframes = {}\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    df_list = []\n",
    "    \n",
    "    for file_id in file_ids:\n",
    "        file_path = f'{bucket}/data/log_reg_{ethnicity}_{file_id}.tsv.bgz'\n",
    "        local_file_name = f'log_reg_{ethnicity}_{file_id}{file_extension}.bgz'\n",
    "        \n",
    "        decompressed_file = load_files(file_path, local_file_name)\n",
    "        df = pd.read_csv(decompressed_file, sep=\"\\t\")\n",
    "        df_cleaned = clean_df(df)\n",
    "        df_list.append(df_cleaned)\n",
    "        \n",
    "        print(f\"Processed file: {decompressed_file}\")\n",
    "    \n",
    "    # Concatenate all DataFrames for this ethnicity\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_dataframes[ethnicity] = merged_df\n",
    "    \n",
    "    # Save merged DataFrame to file\n",
    "    output_file = f'merged_{ethnicity}.tsv'\n",
    "    merged_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "    print(f\"Merged files for {ethnicity}, cleaned, and saved as '{output_file}'.\")\n",
    "\n",
    "#View\n",
    "print(merged_dataframes[\"eur\"].head())\n",
    "print(merged_dataframes[\"afr\"].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in full files if created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg_eur_path = f'{bucket}/data/log_reg_eur.tsv.bgz'\n",
    "# !gsutil cp {log_reg_eur_path} .\n",
    "# !bgzip -d log_reg_eur.tsv.bgz \n",
    "\n",
    "# log_reg_eas_path = f'{bucket}/data/log_reg_eas.tsv.bgz'\n",
    "# !gsutil cp {log_reg_eas_path} .\n",
    "# !bgzip -d log_reg_eas.tsv.bgz \n",
    "\n",
    "# log_reg_afr_path = f'{bucket}/data/log_reg_afr.tsv.bgz'\n",
    "# !gsutil cp {log_reg_afr_path} .\n",
    "# !bgzip -d log_reg_afr.tsv.bgz \n",
    "\n",
    "# log_reg_amr_path = f'{bucket}/data/log_reg_amr.tsv.bgz'\n",
    "# !gsutil cp {log_reg_amr_path} .\n",
    "# !bgzip -d log_reg_amr.tsv.bgz \n",
    "\n",
    "# log_reg_sas_path = f'{bucket}/data/log_reg_sas.tsv.bgz'\n",
    "# !gsutil cp {log_reg_sas_path} .\n",
    "# !bgzip -d log_reg_sas.tsv.bgz \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Finngen GWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_finn_path = f'{bucket}/data/fixed_menisc_sumstats.tsv.bgz'\n",
    "!gsutil cp {log_reg_finn_path} .\n",
    "!bgzip -d fixed_menisc_sumstats.tsv.bgz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process summary statistics files so that they are legible to METAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'eur': 'merged_eur.tsv',\n",
    "    'afr': 'merged_afr.tsv',\n",
    "    'amr': 'merged_amr.tsv',\n",
    "    'eas': 'merged_eas.tsv',\n",
    "    'sas': 'merged_sas.tsv'\n",
    "}\n",
    "\n",
    "# Process each file except 'finn'\n",
    "for key, file in files.items():\n",
    "    df = pd.read_csv(files[key], sep='\\t')\n",
    "    \n",
    "    df = df.rename(columns={'locus': 'position'})\n",
    "    \n",
    "    df['alleles'] = df['alleles'].str.replace(r'[\\[\\]\"]', '', regex=True)\n",
    "    df['ref'] = [a.split(',')[0] if ',' in a else None for a in df['alleles']]\n",
    "    df['alt'] = [a.split(',')[1] if ',' in a else None for a in df['alleles']]\n",
    "    \n",
    "    # Create 'locus' column as 'position_ref_alt'\n",
    "    df['locus'] = df['position'] + '_' + df['ref'] + '_' + df['alt']\n",
    "    \n",
    "    df.dropna(subset=['locus', 'position', 'ref', 'alt', 'beta', 'p_value', 'standard_error'], inplace=True)\n",
    "    \n",
    "    df.to_csv(files[key], sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save completed files to workspace bucket if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bgzip -@ 6 -l 9 -c merged_eur.tsv > merged_eur.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/merged_eur.tsv.bgz'\n",
    "!gsutil cp 'merged_eur.tsv.bgz' {meta_save_path}\n",
    "\n",
    "!bgzip -@ 6 -l 9 -c merged_afr.tsv > merged_afr.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/merged_afr.tsv.bgz'\n",
    "!gsutil cp 'merged_afr.tsv.bgz' {meta_save_path}\n",
    "\n",
    "!bgzip -@ 6 -l 9 -c merged_amr.tsv > merged_amr.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/merged_amr.tsv.bgz'\n",
    "!gsutil cp 'merged_amr.tsv.bgz' {meta_save_path}\n",
    "\n",
    "!bgzip -@ 6 -l 9 -c merged_eas.tsv > merged_eas.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/merged_eas.tsv.bgz'\n",
    "!gsutil cp 'merged_eas.tsv.bgz' {meta_save_path}\n",
    "\n",
    "!bgzip -@ 6 -l 9 -c merged_sas.tsv > merged_sas.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/merged_sas.tsv.bgz'\n",
    "!gsutil cp 'merged_sas.tsv.bgz' {meta_save_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Finngen if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finn = pd.read_csv('fixed_menisc_sumstats.tsv', sep='\\t')\n",
    "df_finn.head()\n",
    "df_finn['position'] = 'chr' + df_finn['chrom'].astype(str) + ':' + df_finn['pos'].astype(str)\n",
    "df_finn['locus'] = df_finn['position'] + '_' + df_finn['ref'] + '_' + df_finn['alt']\n",
    "df_finn = df_finn.rename(columns={'beta_menisc_fixed': 'beta', 'sebeta_menisc_fixed': 'standard_error'})\n",
    "output_file_finn = 'processed_finn.tsv'\n",
    "df_finn.to_csv(output_file_finn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_save_path = f'{bucket}/data/processed_finn.tsv'\n",
    "!gsutil cp 'processed_finn.tsv' {meta_save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform meta-analysis using METAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag out the finngen line if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the contents of the meta_analysis.txt command file\n",
    "metal_command = \"\"\"\n",
    "# Set the analysis scheme to STDERR (standard error-based meta-analysis)\n",
    "SCHEME STDERR\n",
    "COLUMNCOUNTING LENIENT\n",
    "\n",
    "# Define the columns in the input files\n",
    "MARKER locus  # SNP ID\n",
    "ALLELE ref alt  # Alleles\n",
    "EFFECT beta  # Effect size (regression coefficient)\n",
    "STDERR standard_error  # Standard error of the effect size\n",
    "PVALUE p_value  # P-value\n",
    "\n",
    "# Enable genomic control correction if desired\n",
    "# GENOMICCONTROL ON\n",
    "\n",
    "# Specify the files to be processed\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/merged_eur.tsv\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/merged_afr.tsv\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/merged_amr.tsv\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/merged_eas.tsv\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/merged_sas.tsv\n",
    "#Finngen- can be hashtagged out\n",
    "PROCESS /home/jupyter/workspaces/flagshipgwas/processed_finn.tsv\n",
    "\n",
    "\n",
    "# Output the meta-analysis results\n",
    "OUTFILE meta_allGC .tbl\n",
    "\n",
    "# Perform the meta-analysis\n",
    "ANALYZE\n",
    "\"\"\"\n",
    "\n",
    "# Create and write the content to meta_analysis.txt\n",
    "with open(\"meta_analysis.txt\", \"w\") as f:\n",
    "    f.write(metal_command)\n",
    "\n",
    "print(\"meta_analysis.txt file has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./generic-metal/metal meta_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_menisc_data.to_csv(\"fixed_menisc_sumstats.tsv\", sep='\\t', index=False)\n",
    "!bgzip -@ 6 -l 9 -c meta_allGC1.tbl > meta_all1_GC.tsv.bgz\n",
    "meta_save_path = f'{bucket}/data/meta_all1_GC.tsv.bgz'\n",
    "!gsutil cp meta_all1_GC.tsv.bgz {meta_save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
